{"cells":[{"cell_type":"markdown","source":[" # Carregamento da Bronze para a Silver\n"," \n"," Este notebook realiza as seguintes etapas:\n"," \n"," 1. Lê os dados já processados na camada Bronze (tabelas Delta).\n"," 2. Aplica transformações adicionais, incluindo:\n","    - A criação (ou recalculação) de uma chave derivada para cada registro.\n","    - A inclusão de um campo `update_date` com o timestamp atual.\n"," 3. Carrega os dados na camada Silver:\n","    - Se a tabela Silver ainda não existir, ela é criada.\n","    - Se a tabela Silver já existir, é realizado um merge (atualizando registros existentes e inserindo os novos) com base na chave derivada.\n","\n"," **Observações:**\n"," - As funções `flatten_df` e `add_derived_key` são utilizadas para preparar os dados.\n"," - As variáveis `bronze_schema` e `silver_schema` definem os nomes dos schemas para Bronze e Silver, respectivamente.\n"," - Ajuste as chaves de merge se necessário; neste exemplo usamos a coluna `derived_key`.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e8c83fbe-4091-4f2a-aa36-37744a4fcdc6"},{"cell_type":"code","source":["\n","# Exemplo de definição dos schemas Bronze e Silver\n","bronze_schema = \"bronze_health\"\n","silver_schema = \"silver_health\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"99d244f6-a021-4341-b218-aea418c7c1e4"},{"cell_type":"code","source":["from pyspark.sql.functions import col, sha2, concat_ws\n","from delta.tables import DeltaTable\n","\n","# Função para \"achatar\" estruturas aninhadas (ajuste conforme sua necessidade)\n","def flatten_df(nested_df):\n","    flat_cols = []\n","    for column in nested_df.columns:\n","        dtype = nested_df.schema[column].dataType\n","        if hasattr(dtype, \"fields\"):  # se for um StructType\n","            for field in dtype.fields:\n","                flat_cols.append(col(f\"{column}.{field.name}\").alias(f\"{column}_{field.name}\"))\n","        else:\n","            flat_cols.append(col(column))\n","    return nested_df.select(flat_cols)\n","\n","# Função para adicionar uma chave derivada a partir do hash SHA-256 de todas as colunas,\n","# desconsiderando a coluna \"update_date\" se ela existir\n","def add_derived_key(df):\n","    cols_for_key = [col(c).cast(\"string\") for c in df.columns if c != \"update_date\"]\n","    concatenated_cols = concat_ws(\"||\", *cols_for_key)\n","    return df.withColumn(\"derived_key\", sha2(concatenated_cols, 256))\n","\n","# Função para ler a tabela Bronze, transformar os dados (adicionando a chave derivada, sem alterar update_date)\n","# e carregar na camada Silver realizando merge se a tabela já existir, ou criando-a caso contrário\n","def load_silver_table(bronze_table_name, silver_table_name):\n","    # Lê os dados da tabela Bronze\n","    df_bronze = spark.table(bronze_table_name)\n","    \n","    # Aplica flatten (caso os dados não estejam achatados)\n","    df_flat = flatten_df(df_bronze)\n","    \n","    # Adiciona a chave derivada (a coluna update_date já existente não será considerada na geração da chave)\n","    df_transformed = add_derived_key(df_flat)\n","    \n","    # Se a tabela Silver já existe, realiza merge com base na chave derivada; caso contrário, cria a tabela\n","    if spark.catalog.tableExists(silver_table_name):\n","        deltaTable = DeltaTable.forName(spark, silver_table_name)\n","        deltaTable.alias(\"tgt\").merge(\n","            df_transformed.alias(\"src\"),\n","            \"tgt.derived_key = src.derived_key\"\n","        ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n","        print(f\"Tabela Silver '{silver_table_name}' atualizada via merge!\")\n","    else:\n","        df_transformed.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table_name)\n","        print(f\"Tabela Silver '{silver_table_name}' criada com sucesso!\")\n","\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"99c166e5-01f8-4c75-b93e-d504ce3b0909","normalized_state":"finished","queued_time":"2025-02-23T18:01:31.3816271Z","session_start_time":null,"execution_start_time":"2025-02-23T18:01:31.5125184Z","execution_finish_time":"2025-02-23T18:01:31.7672688Z","parent_msg_id":"a66b69c7-d917-498e-8e4e-77f57cf425a0"},"text/plain":"StatementMeta(, 99c166e5-01f8-4c75-b93e-d504ce3b0909, 18, Finished, Available, Finished)"},"metadata":{}}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27834f9b-1ff6-4e57-b78f-79525684566b"},{"cell_type":"markdown","source":["## Configurações de Schemas e Tabelas\n","\n","Definimos os schemas para as camadas Bronze e Silver e os mapeamentos dos tipos de recursos para os nomes das tabelas.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"48350c0d-8057-449b-855f-8dad4007727e"},{"cell_type":"code","source":["\n","\n","# Dicionários com os tipos de recursos e os respectivos nomes das tabelas para Bronze e Silver\n","bronze_resources_tables = {\n","    \"Patient\": f\"{bronze_schema}.Patients\",\n","    \"Encounter\": f\"{bronze_schema}.Encounters\",\n","    \"Condition\": f\"{bronze_schema}.Conditions\",\n","    \"DiagnosticReport\": f\"{bronze_schema}.DiagnosticReports\",\n","    \"DocumentReference\": f\"{bronze_schema}.DocumentReferences\",\n","    \"Claim\": f\"{bronze_schema}.Claims\",\n","    \"ExplanationOfBenefit\": f\"{bronze_schema}.Explanations\",\n","    \"MedicationRequest\": f\"{bronze_schema}.MedicationRequests\",\n","    \"Device\": f\"{bronze_schema}.Devices\",\n","    \"SupplyDelivery\": f\"{bronze_schema}.SupplyDeliveries\",\n","    \"CareTeam\": f\"{bronze_schema}.CareTeams\",\n","    \"CarePlan\": f\"{bronze_schema}.CarePlans\",\n","    \"Observation\": f\"{bronze_schema}.Observations\",\n","    \"Procedure\": f\"{bronze_schema}.Procedures\",\n","    \"Medication\": f\"{bronze_schema}.Medications\",\n","    \"MedicationAdministration\": f\"{bronze_schema}.MedicationAdministrations\",\n","    \"Immunization\": f\"{bronze_schema}.Immunizations\",\n","    \"ImagingStudy\": f\"{bronze_schema}.ImagingStudies\",\n","    \"Provenance\": f\"{bronze_schema}.Provenances\"\n","}\n","\n","silver_resources_tables = {\n","    \"Patient\": f\"{silver_schema}.Patients\",\n","    \"Encounter\": f\"{silver_schema}.Encounters\",\n","    \"Condition\": f\"{silver_schema}.Conditions\",\n","    \"DiagnosticReport\": f\"{silver_schema}.DiagnosticReports\",\n","    \"DocumentReference\": f\"{silver_schema}.DocumentReferences\",\n","    \"Claim\": f\"{silver_schema}.Claims\",\n","    \"ExplanationOfBenefit\": f\"{silver_schema}.Explanations\",\n","    \"MedicationRequest\": f\"{silver_schema}.MedicationRequests\",\n","    \"Device\": f\"{silver_schema}.Devices\",\n","    \"SupplyDelivery\": f\"{silver_schema}.SupplyDeliveries\",\n","    \"CareTeam\": f\"{silver_schema}.CareTeams\",\n","    \"CarePlan\": f\"{silver_schema}.CarePlans\",\n","    \"Observation\": f\"{silver_schema}.Observations\",\n","    \"Procedure\": f\"{silver_schema}.Procedures\",\n","    \"Medication\": f\"{silver_schema}.Medications\",\n","    \"MedicationAdministration\": f\"{silver_schema}.MedicationAdministrations\",\n","    \"Immunization\": f\"{silver_schema}.Immunizations\",\n","    \"ImagingStudy\": f\"{silver_schema}.ImagingStudies\",\n","    \"Provenance\": f\"{silver_schema}.Provenances\"\n","}\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"99c166e5-01f8-4c75-b93e-d504ce3b0909","normalized_state":"finished","queued_time":"2025-02-23T18:01:31.4506232Z","session_start_time":null,"execution_start_time":"2025-02-23T18:01:31.8931693Z","execution_finish_time":"2025-02-23T18:01:32.180189Z","parent_msg_id":"159159e5-96ad-40e8-859c-5612ecfb2b05"},"text/plain":"StatementMeta(, 99c166e5-01f8-4c75-b93e-d504ce3b0909, 19, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"27e857cd-ecbc-461f-b484-97b3ccc72934"},{"cell_type":"markdown","source":["## Processamento: Do Bronze para o Silver\n","\n"," Para cada recurso, lê-se a tabela Bronze correspondente, transforma os dados (adicionando a chave derivada e a data de atualização) e carrega na camada Silver via merge (se a tabela já existir) ou criando a tabela.\n"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fbc76015-df6e-46ae-9cce-e5f5b5a0ccaa"},{"cell_type":"code","source":["# Itera sobre os tipos de recurso, lendo a tabela Bronze e realizando merge/criação na Silver\n","for resource_type in bronze_resources_tables.keys():\n","    bronze_table = bronze_resources_tables[resource_type]\n","    silver_table = silver_resources_tables[resource_type]\n","    load_silver_table(bronze_table, silver_table)\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"finished","livy_statement_state":"cancelled","session_id":"99c166e5-01f8-4c75-b93e-d504ce3b0909","normalized_state":"cancelled","queued_time":"2025-02-23T18:01:31.5306609Z","session_start_time":null,"execution_start_time":"2025-02-23T18:01:32.320377Z","execution_finish_time":"2025-02-23T18:01:57.684795Z","parent_msg_id":"466ca5eb-620b-4761-a08c-57574ecb6f8d"},"text/plain":"StatementMeta(, 99c166e5-01f8-4c75-b93e-d504ce3b0909, 20, Finished, Cancelled, Cancelled)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Tabela Silver 'silver_health.Patients' atualizada via merge!\nTabela Silver 'silver_health.Encounters' atualizada via merge!\nTabela Silver 'silver_health.Conditions' atualizada via merge!\n"]},{"output_type":"error","ename":"Py4JJavaError","evalue":"An error occurred while calling o28547.execute.\n: org.apache.spark.SparkException: Job 672 cancelled part of cancelled job group 20\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2810)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1208)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3073)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3062)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1000)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2563)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$1(DeltaFileFormatWriter.scala:273)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.writeAndCommit(DeltaFileFormatWriter.scala:305)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeWrite(DeltaFileFormatWriter.scala:244)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:224)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$3(TransactionalWrite.scala:542)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:497)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:444)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:274)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:273)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.writeFiles(MergeIntoCommandBase.scala:308)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.writeFiles$(MergeIntoCommandBase.scala:293)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.writeFiles(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.writeFilesInternal$1(LowShuffleMergeExecutor.scala:333)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.$anonfun$writeUnmodifiedRows$6(LowShuffleMergeExecutor.scala:360)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.delta.DeltaTableUtils$.withActiveSession(DeltaTable.scala:499)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.$anonfun$writeUnmodifiedRows$1(LowShuffleMergeExecutor.scala:360)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:426)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:443)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:443)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:440)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:404)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.writeUnmodifiedRows(LowShuffleMergeExecutor.scala:313)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMerge$8(LowShuffleMergeExecutor.scala:164)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:164)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:114)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runLowShuffleMerge(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:128)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:86)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:227)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:86)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:84)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.runOrig(MergeIntoCommandBase.scala:180)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:156)\n\tat org.apache.spark.sql.delta.sources.SQLConfUtils$.withGlutenDisabled(SQLConfUtils.scala:39)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:156)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:152)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:61)\n\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$2(DeltaMergeBuilder.scala:326)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.delta.DeltaTableUtils$.withActiveSession(DeltaTable.scala:499)\n\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:295)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:109)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:95)\n\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:152)\n\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:293)\n\tat jdk.internal.reflect.GeneratedMethodAccessor366.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m bronze_table \u001b[38;5;241m=\u001b[39m bronze_resources_tables[resource_type]\n\u001b[1;32m      4\u001b[0m silver_table \u001b[38;5;241m=\u001b[39m silver_resources_tables[resource_type]\n\u001b[0;32m----> 5\u001b[0m load_silver_table(bronze_table, silver_table)\n","Cell \u001b[0;32mIn[29], line 41\u001b[0m, in \u001b[0;36mload_silver_table\u001b[0;34m(bronze_table_name, silver_table_name)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spark\u001b[38;5;241m.\u001b[39mcatalog\u001b[38;5;241m.\u001b[39mtableExists(silver_table_name):\n\u001b[1;32m     37\u001b[0m     deltaTable \u001b[38;5;241m=\u001b[39m DeltaTable\u001b[38;5;241m.\u001b[39mforName(spark, silver_table_name)\n\u001b[1;32m     38\u001b[0m     deltaTable\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmerge(\n\u001b[1;32m     39\u001b[0m         df_transformed\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msrc\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt.derived_key = src.derived_key\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 41\u001b[0m     )\u001b[38;5;241m.\u001b[39mwhenMatchedUpdateAll()\u001b[38;5;241m.\u001b[39mwhenNotMatchedInsertAll()\u001b[38;5;241m.\u001b[39mexecute()\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTabela Silver \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msilver_table_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m atualizada via merge!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/usr/hdp/current/spark3-client/jars/delta-spark_2.12-3.2.0.5.jar/delta/tables.py:1065\u001b[0m, in \u001b[0;36mDeltaMergeBuilder.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m0.4\u001b[39m)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m    Execute the merge operation based on the built matched and not matched actions.\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \n\u001b[1;32m   1063\u001b[0m \u001b[38;5;124;03m    See :py:class:`~delta.tables.DeltaMergeBuilder` for complete usage details.\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1065\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jbuilder\u001b[38;5;241m.\u001b[39mexecute()\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n","File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n","File \u001b[0;32m~/cluster-env/trident_env/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o28547.execute.\n: org.apache.spark.SparkException: Job 672 cancelled part of cancelled job group 20\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2935)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:2810)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:1208)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3095)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3073)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3062)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1000)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2563)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.$anonfun$executeWrite$1(DeltaFileFormatWriter.scala:273)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.writeAndCommit(DeltaFileFormatWriter.scala:305)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.executeWrite(DeltaFileFormatWriter.scala:244)\n\tat org.apache.spark.sql.delta.files.DeltaFileFormatWriter$.write(DeltaFileFormatWriter.scala:224)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.$anonfun$writeFiles$3(TransactionalWrite.scala:542)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:132)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:220)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:101)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:68)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:497)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:444)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles(TransactionalWrite.scala:274)\n\tat org.apache.spark.sql.delta.files.TransactionalWrite.writeFiles$(TransactionalWrite.scala:273)\n\tat org.apache.spark.sql.delta.OptimisticTransaction.writeFiles(OptimisticTransaction.scala:147)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.writeFiles(MergeIntoCommandBase.scala:308)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.writeFiles$(MergeIntoCommandBase.scala:293)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.writeFiles(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.writeFilesInternal$1(LowShuffleMergeExecutor.scala:333)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.$anonfun$writeUnmodifiedRows$6(LowShuffleMergeExecutor.scala:360)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.delta.DeltaTableUtils$.withActiveSession(DeltaTable.scala:499)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.$anonfun$writeUnmodifiedRows$1(LowShuffleMergeExecutor.scala:360)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.executeThunk$1(MergeIntoCommandBase.scala:426)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$7(MergeIntoCommandBase.scala:443)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$recordMergeOperation$6(MergeIntoCommandBase.scala:443)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.recordMergeOperation(MergeIntoCommandBase.scala:440)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.recordMergeOperation$(MergeIntoCommandBase.scala:404)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordMergeOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.writeUnmodifiedRows(LowShuffleMergeExecutor.scala:313)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.$anonfun$runLowShuffleMerge$8(LowShuffleMergeExecutor.scala:164)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withJobDescription(DeltaProgressReporter.scala:53)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode(DeltaProgressReporter.scala:32)\n\tat org.apache.spark.sql.delta.util.DeltaProgressReporter.withStatusCode$(DeltaProgressReporter.scala:27)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.withStatusCode(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge(LowShuffleMergeExecutor.scala:164)\n\tat org.apache.spark.sql.delta.commands.merge.LowShuffleMergeExecutor.runLowShuffleMerge$(LowShuffleMergeExecutor.scala:114)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runLowShuffleMerge(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2(MergeIntoCommand.scala:128)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$2$adapted(MergeIntoCommand.scala:86)\n\tat org.apache.spark.sql.delta.DeltaLog.withNewTransaction(DeltaLog.scala:227)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.$anonfun$runMerge$1(MergeIntoCommand.scala:86)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:169)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:167)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordFrameProfile(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:137)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation(SynapseLoggingShim.scala:111)\n\tat com.microsoft.spark.telemetry.delta.SynapseLoggingShim.recordOperation$(SynapseLoggingShim.scala:93)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:136)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:126)\n\tat org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:116)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.recordDeltaOperation(MergeIntoCommand.scala:61)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.runMerge(MergeIntoCommand.scala:84)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.runOrig(MergeIntoCommandBase.scala:180)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.$anonfun$run$1(MergeIntoCommandBase.scala:156)\n\tat org.apache.spark.sql.delta.sources.SQLConfUtils$.withGlutenDisabled(SQLConfUtils.scala:39)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.run(MergeIntoCommandBase.scala:156)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommandBase.run$(MergeIntoCommandBase.scala:152)\n\tat org.apache.spark.sql.delta.commands.MergeIntoCommand.run(MergeIntoCommand.scala:61)\n\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$2(DeltaMergeBuilder.scala:326)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.delta.DeltaTableUtils$.withActiveSession(DeltaTable.scala:499)\n\tat io.delta.tables.DeltaMergeBuilder.$anonfun$execute$1(DeltaMergeBuilder.scala:295)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError(AnalysisHelper.scala:109)\n\tat org.apache.spark.sql.delta.util.AnalysisHelper.improveUnsupportedOpError$(AnalysisHelper.scala:95)\n\tat io.delta.tables.DeltaMergeBuilder.improveUnsupportedOpError(DeltaMergeBuilder.scala:152)\n\tat io.delta.tables.DeltaMergeBuilder.execute(DeltaMergeBuilder.scala:293)\n\tat jdk.internal.reflect.GeneratedMethodAccessor366.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"]}],"execution_count":18,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"432f4de6-60f6-4cc3-8dc2-7bbfb220e9b5"},{"cell_type":"code","source":["%%sql\n","select count(*) from silver_health.Encounters limit 2--703"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"99c166e5-01f8-4c75-b93e-d504ce3b0909","normalized_state":"finished","queued_time":"2025-02-23T18:02:00.2428872Z","session_start_time":null,"execution_start_time":"2025-02-23T18:02:00.5498403Z","execution_finish_time":"2025-02-23T18:02:02.0852454Z","parent_msg_id":"fdb34357-e87e-4f70-a148-07b69f0eb4c7"},"text/plain":"StatementMeta(, 99c166e5-01f8-4c75-b93e-d504ce3b0909, 22, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":20,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"count(1)","type":"long","nullable":false,"metadata":{"__autoGeneratedAlias":"true"}}]},"data":[["1406"]]},"text/plain":"<Spark SQL result set with 1 rows and 1 fields>"},"metadata":{}}],"execution_count":20,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"987ca9f0-8944-48e7-8453-2d12946d1b22"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}